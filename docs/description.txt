pubCrawl is a distributed systems programming language with a focus on 
list operations. Given some number of networked slave computers, it will
automatically split work up among them, requiring the developer only to specify
whether a function should distribute work or not. To create a robust platform, we
will also implement redundant processing, so that processed data is 
not lost in the case of an unstable network.

To distribute work, the developer will specify the IP addresses of the slave 
computers, which will be running a slave server utility. When the output program is run, 
it sends compiled and serialized functions to the remote servers. It then passes data to each slave 
computer as the functions are called. This is blocking - the
program does not continue until all the results have been received.

What problems can PubCrawl solve and how should it be used?

PubCrawl is a language designed to solve problems where you have a lot of data. Unlike multi-core processing, Pubcrawl helps you distribute your computationally extensive problem across multiple machines in a cluster. Those machines will be able to chip away at their portion of the problem, and then send the results back to the master machine. A simple analogy for this type of problem would be counting the number of books in the library. It would be much faster to have two people counting (one person counts the even numbered shelves, and the other counting the odd shelves). When finished, they just combine their numbers to produce the result.

One concrete example may be counting the appearance of each word in a large set of documents. With PubCrawl, you would easily be able to tokenize the documents (split each document by word), and pass each word to the distributor. You could then implement a function to count instances of each word. This function would then be distributed and run parallel across the cluster. Each node could then count the number of items in each list and then return the result to the master node to sum up.
